import streamlit as st
import os
import tempfile
from rag_pipeline import build_rag_chain  # normalize_text da dÃ¶necek

# ğŸ¨ Sayfa yapÄ±landÄ±rmasÄ±
st.set_page_config(page_title="Ä°BB Faaliyet Raporu RAG", layout="wide")

# ğŸ™ï¸ BaÅŸlÄ±k
st.title("ğŸ“˜ Faaliyet Raporu RAG UygulamasÄ±")
st.caption("Google Gemini + Hugging Face + LangChain + Streamlit")

# ğŸ§­ Sidebar
st.sidebar.header("âš™ï¸ Uygulama Bilgileri")
st.sidebar.write("**LLM:** Gemini 2.5 Flash")
st.sidebar.write("**Embedding Model:** paraphrase-multilingual-mpnet-base-v2")
st.sidebar.write("**VektÃ¶r VeritabanÄ±:** FAISS (her seferinde yeniden oluÅŸturuluyor)")
st.sidebar.write("---")

# ğŸš¨ API AnahtarÄ± kontrolÃ¼
if os.getenv("GEMINI_API_KEY") is None:
    st.error("âŒ LÃ¼tfen `GEMINI_API_KEY` ortam deÄŸiÅŸkenini ayarlayÄ±n.")
    st.stop()
else:
    st.sidebar.success("âœ… API AnahtarÄ± bulundu.")

# ğŸ“ Dosya yÃ¼kleme
uploaded_file = st.file_uploader(
    "Bir .pdf veya .txt dosyasÄ± yÃ¼kleyin (Ã–rn: 2024_Faaliyet_Raporu.pdf)",
    type=["pdf", "txt"]
)

# ğŸ§© Model kurulum
if uploaded_file is not None:
    suffix = os.path.splitext(uploaded_file.name)[1]
    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_file:
        tmp_file.write(uploaded_file.read())
        temp_file_path = tmp_file.name

    st.success(f"âœ… Dosya baÅŸarÄ±yla yÃ¼klendi: {uploaded_file.name}")

    # Zinciri oluÅŸtur
    if "qa_chain" not in st.session_state:
        with st.spinner("ğŸ”§ Metin bÃ¶lÃ¼nÃ¼yor, embedding oluÅŸturuluyor ve model hazÄ±rlanÄ±yor..."):
            try:
                qa_chain, normalize_text = build_rag_chain(temp_file_path)
                st.session_state.qa_chain = qa_chain
                st.session_state.normalize_text = normalize_text
                st.success("ğŸš€ Model baÅŸarÄ±yla hazÄ±rlandÄ±! ArtÄ±k sorular sorabilirsiniz.")
            except Exception as e:
                st.error(f"âŒ Model hazÄ±rlanÄ±rken bir hata oluÅŸtu: {e}")
                st.stop()
            finally:
                os.unlink(temp_file_path)

    # ğŸ§  Soru sorma alanÄ±
    st.markdown("### ğŸ” Soru Sorun")

    if "messages" not in st.session_state:
        st.session_state.messages = []
        st.session_state.messages.append(
            {"role": "assistant", "content": "RAG zinciri kuruldu. LÃ¼tfen raporunuzla ilgili sorularÄ±nÄ±zÄ± sorun."}
        )

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    user_query = st.chat_input("Raporla ilgili sorunuzu yazÄ±n:")

    if user_query:
        with st.chat_message("user"):
            st.markdown(user_query)
        st.session_state.messages.append({"role": "user", "content": user_query})

        with st.chat_message("assistant"):
            with st.spinner("ğŸ’¬ Model yanÄ±t Ã¼retiyor..."):
                try:
                    qa_chain = st.session_state.qa_chain
                    normalize_text = st.session_state.normalize_text

                    # âœ… Sorguyu normalize et (TÃ¼rkÃ§e karakter farkÄ± giderilir)
                    normalized_query = normalize_text(user_query)
                    response = qa_chain.invoke(normalized_query)
                    answer = response.get("result", "Cevap bulunamadÄ±.")

                    st.markdown(answer)
                    st.session_state.messages.append({"role": "assistant", "content": answer})

                except Exception as e:
                    error_message = f"âŒ YanÄ±t Ã¼retirken bir hata oluÅŸtu: {e}"
                    st.error(error_message)
                    st.session_state.messages.append({"role": "assistant", "content": error_message})

else:
    if "qa_chain" in st.session_state:
        del st.session_state.qa_chain
    st.info("â¬†ï¸ LÃ¼tfen .pdf veya .txt dosyasÄ±nÄ± yÃ¼kleyin.")
